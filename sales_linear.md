<h1 align="center">Linear Regression</h1>

----

$$ 
M(x_1,x_2,x_3) = 1937.964582 -5.157235 x_1 + 496.6025 x_2 + 1116.807677 x_3 
$$ 


$$
\begin{align}
x_1^{[1]} &= 201.8647, & x_2^{[1]} &= 1.0, & x_3^{[1]} &= 1.0, \\
x_1^{[2]} &= 398.9774, & x_2^{[2]} &= 0.0, & x_3^{[2]} &= 1.2, \\
x_1^{[3]} &= 330.7531, & x_2^{[3]} &= 1.0, & x_3^{[3]} &= 1.0, \\
x_1^{[4]} &= 66.0156, & x_2^{[4]} &= 0.0, & x_3^{[4]} &= 0.8, \\
x_1^{[5]} &= 309.7027, & x_2^{[5]} &= 0.0, & x_3^{[5]} &= 1.2, \\
x_1^{[6]} &= 161.3583, & x_2^{[6]} &= 1.0, & x_3^{[6]} &= 1.2, \\
x_1^{[7]} &= 116.1831, & x_2^{[7]} &= 1.0, & x_3^{[7]} &= 0.8, \\
x_1^{[8]} &= 339.2498, & x_2^{[8]} &= 1.0, & x_3^{[8]} &= 0.8, \\
x_1^{[9]} &= 261.4119, & x_2^{[9]} &= 0.0, & x_3^{[9]} &= 1.2, \\
x_1^{[10]} &= 86.607, & x_2^{[10]} &= 0.0, & x_3^{[10]} &= 1.0, \\
x_1^{[11]} &= 224.6746, & x_2^{[11]} &= 0.0, & x_3^{[11]} &= 1.2, \\
x_1^{[12]} &= 482.6379, & x_2^{[12]} &= 0.0, & x_3^{[12]} &= 1.0, \\
x_1^{[13]} &= 184.0367, & x_2^{[13]} &= 0.0, & x_3^{[13]} &= 1.2, \\
x_1^{[14]} &= 208.6165, & x_2^{[14]} &= 1.0, & x_3^{[14]} &= 1.0, \\
x_1^{[15]} &= 122.6153, & x_2^{[15]} &= 1.0, & x_3^{[15]} &= 0.8, \\
x_1^{[16]} &= 155.1607, & x_2^{[16]} &= 0.0, & x_3^{[16]} &= 0.8, \\
x_1^{[17]} &= 271.6781, & x_2^{[17]} &= 1.0, & x_3^{[17]} &= 1.2, \\
x_1^{[18]} &= 420.4519, & x_2^{[18]} &= 0.0, & x_3^{[18]} &= 1.0, \\
x_1^{[19]} &= 287.2648, & x_2^{[19]} &= 0.0, & x_3^{[19]} &= 1.0, \\
x_1^{[20]} &= 231.0326, & x_2^{[20]} &= 1.0, & x_3^{[20]} &= 0.8, \\
x_1^{[21]} &= 372.9922, & x_2^{[21]} &= 1.0, & x_3^{[21]} &= 0.8, \\
x_1^{[22]} &= 101.7198, & x_2^{[22]} &= 0.0, & x_3^{[22]} &= 1.2, \\
x_1^{[23]} &= 475.5874, & x_2^{[23]} &= 0.0, & x_3^{[23]} &= 1.0, \\
x_1^{[24]} &= 330.689, & x_2^{[24]} &= 1.0, & x_3^{[24]} &= 1.0, \\
x_1^{[25]} &= 383.7323, & x_2^{[25]} &= 0.0, & x_3^{[25]} &= 1.0, \\
x_1^{[26]} &= 175.0248, & x_2^{[26]} &= 1.0, & x_3^{[26]} &= 0.8, \\
x_1^{[27]} &= 222.7248, & x_2^{[27]} &= 0.0, & x_3^{[27]} &= 1.2, \\
x_1^{[28]} &= 235.2865, & x_2^{[28]} &= 0.0, & x_3^{[28]} &= 1.2, \\
x_1^{[29]} &= 254.2815, & x_2^{[29]} &= 0.0, & x_3^{[29]} &= 1.0, \\
x_1^{[30]} &= 345.7389, & x_2^{[30]} &= 1.0, & x_3^{[30]} &= 1.2, \\
x_1^{[31]} &= 371.5682, & x_2^{[31]} &= 0.0, & x_3^{[31]} &= 0.8, \\
x_1^{[32]} &= 100.1805, & x_2^{[32]} &= 0.0, & x_3^{[32]} &= 1.2, \\
x_1^{[33]} &= 357.1432, & x_2^{[33]} &= 1.0, & x_3^{[33]} &= 0.8, \\
x_1^{[34]} &= 430.2678, & x_2^{[34]} &= 1.0, & x_3^{[34]} &= 1.0, \\
x_1^{[35]} &= 438.9038, & x_2^{[35]} &= 0.0, & x_3^{[35]} &= 1.0, \\
x_1^{[36]} &= 391.4219, & x_2^{[36]} &= 0.0, & x_3^{[36]} &= 1.0, \\
x_1^{[37]} &= 338.4887, & x_2^{[37]} &= 0.0, & x_3^{[37]} &= 1.2, \\
x_1^{[38]} &= 223.294, & x_2^{[38]} &= 1.0, & x_3^{[38]} &= 1.0, \\
x_1^{[39]} &= 369.2475, & x_2^{[39]} &= 0.0, & x_3^{[39]} &= 1.2, \\
x_1^{[40]} &= 235.408, & x_2^{[40]} &= 1.0, & x_3^{[40]} &= 1.2, \\
x_1^{[41]} &= 72.189, & x_2^{[41]} &= 1.0, & x_3^{[41]} &= 1.2, \\
x_1^{[42]} &= 413.7788, & x_2^{[42]} &= 0.0, & x_3^{[42]} &= 1.0, \\
x_1^{[43]} &= 155.0283, & x_2^{[43]} &= 0.0, & x_3^{[43]} &= 1.2, \\
x_1^{[44]} &= 58.2047, & x_2^{[44]} &= 0.0, & x_3^{[44]} &= 1.2, \\
x_1^{[45]} &= 216.4499, & x_2^{[45]} &= 0.0, & x_3^{[45]} &= 0.8, \\
x_1^{[46]} &= 248.0686, & x_2^{[46]} &= 1.0, & x_3^{[46]} &= 1.2, \\
x_1^{[47]} &= 139.6489, & x_2^{[47]} &= 1.0, & x_3^{[47]} &= 1.2, \\
x_1^{[48]} &= 367.3126, & x_2^{[48]} &= 0.0, & x_3^{[48]} &= 0.8, \\
x_1^{[49]} &= 254.4698, & x_2^{[49]} &= 0.0, & x_3^{[49]} &= 1.2, \\
x_1^{[50]} &= 469.2892, & x_2^{[50]} &= 1.0, & x_3^{[50]} &= 0.8, \\
x_1^{[51]} &= 133.5799, & x_2^{[51]} &= 1.0, & x_3^{[51]} &= 1.0, \\
x_1^{[52]} &= 314.5079, & x_2^{[52]} &= 1.0, & x_3^{[52]} &= 0.8, \\
x_1^{[53]} &= 341.8091, & x_2^{[53]} &= 0.0, & x_3^{[53]} &= 0.8, \\
x_1^{[54]} &= 470.1269, & x_2^{[54]} &= 0.0, & x_3^{[54]} &= 1.2, \\
x_1^{[55]} &= 387.9229, & x_2^{[55]} &= 0.0, & x_3^{[55]} &= 1.2, \\
x_1^{[56]} &= 211.7025, & x_2^{[56]} &= 0.0, & x_3^{[56]} &= 0.8, \\
x_1^{[57]} &= 283.4154, & x_2^{[57]} &= 1.0, & x_3^{[57]} &= 1.0, \\
x_1^{[58]} &= 303.2099, & x_2^{[58]} &= 1.0, & x_3^{[58]} &= 1.2, \\
x_1^{[59]} &= 269.4325, & x_2^{[59]} &= 0.0, & x_3^{[59]} &= 1.0, \\
x_1^{[60]} &= 298.6254, & x_2^{[60]} &= 0.0, & x_3^{[60]} &= 1.2, \\
x_1^{[61]} &= 185.4336, & x_2^{[61]} &= 1.0, & x_3^{[61]} &= 1.2, \\
x_1^{[62]} &= 173.703, & x_2^{[62]} &= 1.0, & x_3^{[62]} &= 1.0, \\
x_1^{[63]} &= 294.8489, & x_2^{[63]} &= 0.0, & x_3^{[63]} &= 1.0, \\
x_1^{[64]} &= 389.998, & x_2^{[64]} &= 1.0, & x_3^{[64]} &= 0.8, \\
x_1^{[65]} &= 339.61, & x_2^{[65]} &= 1.0, & x_3^{[65]} &= 1.0, \\
x_1^{[66]} &= 471.2565, & x_2^{[66]} &= 1.0, & x_3^{[66]} &= 1.2, \\
x_1^{[67]} &= 205.3365, & x_2^{[67]} &= 0.0, & x_3^{[67]} &= 1.2, \\
x_1^{[68]} &= 395.8105, & x_2^{[68]} &= 1.0, & x_3^{[68]} &= 0.8, \\
x_1^{[69]} &= 460.1038, & x_2^{[69]} &= 1.0, & x_3^{[69]} &= 1.0, \\
x_1^{[70]} &= 273.8929, & x_2^{[70]} &= 1.0, & x_3^{[70]} &= 0.8, \\
x_1^{[71]} &= 499.4568, & x_2^{[71]} &= 0.0, & x_3^{[71]} &= 1.2, \\
x_1^{[72]} &= 465.1247, & x_2^{[72]} &= 1.0, & x_3^{[72]} &= 1.0, \\
x_1^{[73]} &= 191.244, & x_2^{[73]} &= 1.0, & x_3^{[73]} &= 1.0, \\
x_1^{[74]} &= 118.2178, & x_2^{[74]} &= 0.0, & x_3^{[74]} &= 1.0, \\
x_1^{[75]} &= 493.1863, & x_2^{[75]} &= 1.0, & x_3^{[75]} &= 1.2, \\
x_1^{[76]} &= 68.1464, & x_2^{[76]} &= 0.0, & x_3^{[76]} &= 1.2, \\
x_1^{[77]} &= 471.5734, & x_2^{[77]} &= 1.0, & x_3^{[77]} &= 0.8, \\
x_1^{[78]} &= 208.2694, & x_2^{[78]} &= 1.0, & x_3^{[78]} &= 0.8, \\
x_1^{[79]} &= 81.4379, & x_2^{[79]} &= 0.0, & x_3^{[79]} &= 1.0, \\
x_1^{[80]} &= 483.5275, & x_2^{[80]} &= 1.0, & x_3^{[80]} &= 1.0, \\
x_1^{[81]} &= 308.9973, & x_2^{[81]} &= 0.0, & x_3^{[81]} &= 0.8, \\
x_1^{[82]} &= 128.9171, & x_2^{[82]} &= 0.0, & x_3^{[82]} &= 0.8, \\
x_1^{[83]} &= 274.2499, & x_2^{[83]} &= 1.0, & x_3^{[83]} &= 1.2, \\
x_1^{[84]} &= 297.7985, & x_2^{[84]} &= 0.0, & x_3^{[84]} &= 1.0, \\
x_1^{[85]} &= 283.1069, & x_2^{[85]} &= 1.0, & x_3^{[85]} &= 1.0, \\
x_1^{[86]} &= 215.802, & x_2^{[86]} &= 0.0, & x_3^{[86]} &= 1.0, \\
x_1^{[87]} &= 207.7342, & x_2^{[87]} &= 1.0, & x_3^{[87]} &= 0.8, \\
x_1^{[88]} &= 457.7392, & x_2^{[88]} &= 1.0, & x_3^{[88]} &= 1.2, \\
x_1^{[89]} &= 286.4122, & x_2^{[89]} &= 1.0, & x_3^{[89]} &= 1.2, \\
x_1^{[90]} &= 276.75, & x_2^{[90]} &= 1.0, & x_3^{[90]} &= 0.8, \\
x_1^{[91]} &= 175.8206, & x_2^{[91]} &= 1.0, & x_3^{[91]} &= 1.2, \\
x_1^{[92]} &= 455.5666, & x_2^{[92]} &= 0.0, & x_3^{[92]} &= 1.2, \\
x_1^{[93]} &= 273.2988, & x_2^{[93]} &= 1.0, & x_3^{[93]} &= 1.2, \\
x_1^{[94]} &= 395.2347, & x_2^{[94]} &= 0.0, & x_3^{[94]} &= 1.2, \\
x_1^{[95]} &= 249.1455, & x_2^{[95]} &= 1.0, & x_3^{[95]} &= 0.8, \\
x_1^{[96]} &= 258.7918, & x_2^{[96]} &= 0.0, & x_3^{[96]} &= 0.8, \\
x_1^{[97]} &= 416.3939, & x_2^{[97]} &= 0.0, & x_3^{[97]} &= 1.0, \\
x_1^{[98]} &= 435.5285, & x_2^{[98]} &= 1.0, & x_3^{[98]} &= 1.2, \\
x_1^{[99]} &= 227.1568, & x_2^{[99]} &= 0.0, & x_3^{[99]} &= 0.8, \\
x_1^{[100]} &= 249.0772, & x_2^{[100]} &= 0.0, & x_3^{[100]} &= 1.2.
\end{align}
$$


$$
\begin{align}
d^{[i]} &= \left(x_1 - x_1^{[i]}\right)^2 + \left(x_2 - x_2^{[i]}\right)^2 + \left(x_3 - x_3^{[i]}\right)^2, \quad i = 1,2,\cdots, n \\
d &= \left[d^{[1]}, d^{[2]}, \cdots, d^{[100]}\right]^T \\
\delta &= \text{min}\_3(d) \\
M(x_1,x_2,x_3) &= \frac{\sum_{i=1}^{m} \mathbb{I}(d^{[i]} \leq \delta) \cdot y^{[i]}}{\sum_{i=1}^{m} \mathbb{I}(d^{[i]} \leq \delta)}
\end{align}
$$

----

To get the model, we just use the simplest way of linear regression. Firstly, we randomly separate the dataset into train and test dataset with a ratio of 7:3. 

- Train Indices: 12,  48,  86,  29,  94,   6,  67,  66,  36,  17,  50,  35,   8,
             96,  28,  20,  82,  26,  63,  14,  25,   4,  18,  39,   9,  79,
              7,  65,  37,  90,  57, 100,  55,  44,  51,  68,  47,  69,  62,
             98,  80,  42,  59,  49,  99,  58,  76,  33,  95,  60,  64,  85,
             38,  30,   2,  53,  22,   3,  24,  88,  92,  75,  87,  83,  21,
             61,  72,  15,  93,  52
- Test Indices: 84, 54, 71, 46, 45, 40, 23, 81, 11,  1, 19, 31, 74, 34, 91,  5, 77,
            78, 13, 32, 56, 89, 27, 43, 70, 16, 41, 97, 10, 73

Then we fit the linear regression model by computing:

$\beta = (X^TX)^{-1}X^Ty$, and round the result into 6 decimal points, which yields that:

$$
M(x_1,x_2,x_3) = 1937.964582 -5.157235 x_1 + 496.6025 x_2 + 1116.807677 x_3
$$

We also compute the evaluation metrics:\
Training Set MAD: 72.18050734762957\
Training Set MSE: 8340.06694071241\
Training Set RMSE: 91.32396695672178\
Training Set R-squared: 0.979544712974249\
Test Set MAD: 74.77093912630646\
Test Set MSE: 8689.619505183051\
Test Set RMSE: 93.21812862948414\
Test Set R-squared: 0.9833155409384102\
Population MAD: 83.3727744983166\
Population MSE: 10883.980736279285\
Population RMSE: 104.32631852164288\
Population R-squared: 0.9791080689152433

âš  Population Metrics are invisible to users. We can compute it because this is artificial dataset
