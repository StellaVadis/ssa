<h1 align="center">K-Nearest Neighbors Regression</h1>

----

$$
\begin{bmatrix}
x_1^{[1]} & x_2^{[1]} & x_3^{[1]} \\
x_1^{[2]} & x_2^{[2]} & x_3^{[2]} \\
\vdots & \vdots & \vdots \\
x_1^{[100]} & x_2^{[100]} & x_3^{[100]}
\end{bmatrix} =
\begin{bmatrix}
201.8647 & 1.0 & 1.0 \\
398.9774 & 0.0 & 1.2 \\
330.7531 & 1.0 & 1.0 \\
66.0156 & 0.0 & 0.8 \\
309.7027 & 0.0 & 1.2 \\
161.3583 & 1.0 & 1.2 \\
116.1831 & 1.0 & 0.8 \\
339.2498 & 1.0 & 0.8 \\
261.4119 & 0.0 & 1.2 \\
86.607 & 0.0 & 1.0 \\
224.6746 & 0.0 & 1.2 \\
482.6379 & 0.0 & 1.0 \\
184.0367 & 0.0 & 1.2 \\
208.6165 & 1.0 & 1.0 \\
122.6153 & 1.0 & 0.8 \\
155.1607 & 0.0 & 0.8 \\
271.6781 & 1.0 & 1.2 \\
420.4519 & 0.0 & 1.0 \\
287.2648 & 0.0 & 1.0 \\
231.0326 & 1.0 & 0.8 \\
372.9922 & 1.0 & 0.8 \\
101.7198 & 0.0 & 1.2 \\
475.5874 & 0.0 & 1.0 \\
330.689 & 1.0 & 1.0 \\
383.7323 & 0.0 & 1.0 \\
175.0248 & 1.0 & 0.8 \\
222.7248 & 0.0 & 1.2 \\
235.2865 & 0.0 & 1.2 \\
254.2815 & 0.0 & 1.0 \\
345.7389 & 1.0 & 1.2 \\
371.5682 & 0.0 & 0.8 \\
100.1805 & 0.0 & 1.2 \\
357.1432 & 1.0 & 0.8 \\
430.2678 & 1.0 & 1.0 \\
438.9038 & 0.0 & 1.0 \\
391.4219 & 0.0 & 1.0 \\
338.4887 & 0.0 & 1.2 \\
223.294 & 1.0 & 1.0 \\
369.2475 & 0.0 & 1.2 \\
235.408 & 1.0 & 1.2 \\
72.189 & 1.0 & 1.2 \\
413.7788 & 0.0 & 1.0 \\
155.0283 & 0.0 & 1.2 \\
58.2047 & 0.0 & 1.2 \\
216.4499 & 0.0 & 0.8 \\
248.0686 & 1.0 & 1.2 \\
139.6489 & 1.0 & 1.2 \\
367.3126 & 0.0 & 0.8 \\
254.4698 & 0.0 & 1.2 \\
469.2892 & 1.0 & 0.8 \\
133.5799 & 1.0 & 1.0 \\
314.5079 & 1.0 & 0.8 \\
341.8091 & 0.0 & 0.8 \\
470.1269 & 0.0 & 1.2 \\
387.9229 & 0.0 & 1.2 \\
211.7025 & 0.0 & 0.8 \\
283.4154 & 1.0 & 1.0 \\
303.2099 & 1.0 & 1.2 \\
269.4325 & 0.0 & 1.0 \\
298.6254 & 0.0 & 1.2 \\
185.4336 & 1.0 & 1.2 \\
173.703 & 1.0 & 1.0 \\
294.8489 & 0.0 & 1.0 \\
389.998 & 1.0 & 0.8 \\
339.61 & 1.0 & 1.0 \\
471.2565 & 1.0 & 1.2 \\
205.3365 & 0.0 & 1.2 \\
395.8105 & 1.0 & 0.8 \\
460.1038 & 1.0 & 1.0 \\
273.8929 & 1.0 & 0.8 \\
499.4568 & 0.0 & 1.2 \\
465.1247 & 1.0 & 1.0 \\
191.244 & 1.0 & 1.0 \\
118.2178 & 0.0 & 1.0 \\
493.1863 & 1.0 & 1.2 \\
68.1464 & 0.0 & 1.2 \\
471.5734 & 1.0 & 0.8 \\
208.2694 & 1.0 & 0.8 \\
81.4379 & 0.0 & 1.0 \\
483.5275 & 1.0 & 1.0 \\
308.9973 & 0.0 & 0.8 \\
128.9171 & 0.0 & 0.8 \\
274.2499 & 1.0 & 1.2 \\
297.7985 & 0.0 & 1.0 \\
283.1069 & 1.0 & 1.0 \\
215.802 & 0.0 & 1.0 \\
207.7342 & 1.0 & 0.8 \\
457.7392 & 1.0 & 1.2 \\
286.4122 & 1.0 & 1.2 \\
276.75 & 1.0 & 0.8 \\
175.8206 & 1.0 & 1.2 \\
455.5666 & 0.0 & 1.2 \\
273.2988 & 1.0 & 1.2 \\
395.2347 & 0.0 & 1.2 \\
249.1455 & 1.0 & 0.8 \\
258.7918 & 0.0 & 0.8 \\
416.3939 & 0.0 & 1.0 \\
435.5285 & 1.0 & 1.2 \\
227.1568 & 0.0 & 0.8 \\
249.0772 & 0.0 & 1.2 \\
\end{bmatrix}  \quad \quad \quad
\begin{bmatrix}
y^{[1]} \\
\vdots \\
y^{[100]} 
\end{bmatrix} =
\begin{bmatrix}
2381.0578 \\
1142.7112 \\
1711.2569 \\
2336.814 \\
1742.4426 \\
2921.2039 \\
2570.2996 \\
1402.6437 \\
1951.5808 \\
2749.7522 \\
2067.0371 \\
513.6496 \\
2270.2335 \\
2497.2183 \\
2693.2618 \\
2068.9126 \\
2199.3707 \\
881.2289 \\
1591.4331 \\
2168.9041 \\
1346.168 \\
2775.226 \\
445.8165 \\
1994.4908 \\
1195.5492 \\
2672.0752 \\
2076.2119 \\
2022.5324 \\
1894.6948 \\
1897.5232 \\
855.0858 \\
2714.2537 \\
1461.4461 \\
1359.1252 \\
592.8714 \\
1077.2983 \\
1579.252 \\
2533.5173 \\
1367.4192 \\
2600.7462 \\
3292.6902 \\
1026.5558 \\
2242.2612 \\
2911.2136 \\
1716.9758 \\
2352.3471 \\
3042.4897 \\
842.1088 \\
2070.1185 \\
986.2073 \\
2932.036 \\
1664.9306 \\
1122.1264 \\
779.465 \\
1250.1121 \\
1780.9085 \\
2061.1898 \\
2173.8882 \\
1733.1277 \\
1786.3417 \\
2752.1627 \\
2890.6327 \\
1643.5694 \\
1290.9514 \\
1886.8596 \\
1451.1318 \\
2261.6466 \\
1285.0897 \\
1133.5057 \\
1974.6313 \\
601.8209 \\
1202.1775 \\
2586.3936 \\
2456.7949 \\
1207.4992 \\
2759.6087 \\
967.9286 \\
2260.0668 \\
2684.3561 \\
1009.9462 \\
1326.8698 \\
2140.6596 \\
2461.8144 \\
1349.4814 \\
2034.6993 \\
1945.0026 \\
2218.1682 \\
1437.8644 \\
2292.7539 \\
1940.4283 \\
2955.0304 \\
918.0656 \\
2273.117 \\
1277.1283 \\
2123.8091 \\
1412.6551 \\
1039.8406 \\
1433.5744 \\
1601.6042 \\
2040.7834
\end{bmatrix}
$$



$$
\begin{align}
d^{[i]} &= \left( \frac{x_1 - x_1^{[i]}}{117.412855} \right)^2 + \left(\frac{x_2 - x_2^{[i]}}{0.501757}\right)^2 + \left(\frac{x_3 - x_3^{[i]}}{0.163831}\right)^2, \quad i = 1,2,\cdots, n \\
d &= \left[d^{[1]}, d^{[2]}, \cdots, d^{[100]}\right]^T \\
\delta &= \text{min}\_3(d) \\
M(x_1,x_2,x_3) &= \frac{\sum_{i=1}^{m} \mathbb{I}(d^{[i]} \leq \delta) \cdot y^{[i]}}{\sum_{i=1}^{m} \mathbb{I}(d^{[i]} \leq \delta)}
\end{align}
$$

----


We select $K=3$ and $d(x,x') = ||x-x'||^2$. We keep the same dataset split as in the linear regression model.

Also, we standardize the features by:

$$
\begin{align}
z_1 = \frac{x_1 - \mu_1}{\sigma_1}, \quad \text{where} \quad \mu_1 = \frac{1}{100}\sum_{i=1}^{100}x_1^{[i]}, \quad \sigma_1 = \sqrt{\frac{1}{100}\sum_{i=1}^{100} \left(x_1^{[i]} - \mu_1\right)^2} \\
z_2 = \frac{x_2 - \mu_2}{\sigma_2}, \quad \text{where} \quad \mu_2 = \frac{1}{100}\sum_{i=1}^{100}x_2^{[i]}, \quad \sigma_2 = \sqrt{\frac{1}{100}\sum_{i=1}^{100} \left(x_2^{[i]} - \mu_2\right)^2} \\
z_3 = \frac{x_3 - \mu_3}{\sigma_3}, \quad \text{where} \quad \mu_3 = \frac{1}{100}\sum_{i=1}^{100}x_3^{[i]}, \quad \sigma_3 = \sqrt{\frac{1}{100}\sum_{i=1}^{100} \left(x_3^{[i]} - \mu_3\right)^2}
\end{align}
$$

at here we round the $\sigma_1, \sigma_2, \sigma_3$ to 6 decimal points.

Then we can apply the KNN algorithm to it.

Training set indices: Int64Index([ 12,  48,  86,  29,  94,   6,  67,  66,  36,  17,  50,  35,   8,
             96,  28,  20,  82,  26,  63,  14,  25,   4,  18,  39,   9,  79,
              7,  65,  37,  90,  57, 100,  55,  44,  51,  68,  47,  69,  62,
             98,  80,  42,  59,  49,  99,  58,  76,  33,  95,  60,  64,  85,
             38,  30,   2,  53,  22,   3,  24,  88,  92,  75,  87,  83,  21,
             61,  72,  15,  93,  52],
           dtype='int64')
Test set indices: Int64Index([84, 54, 71, 46, 45, 40, 23, 81, 11,  1, 19, 31, 74, 34, 91,  5, 77,
            78, 13, 32, 56, 89, 27, 43, 70, 16, 41, 97, 10, 73],
           dtype='int64')

           
Training Set MAD: 83.95796952380954 \
Training Set MSE: 11696.233783796577 \
Training Set RMSE: 108.14912752212372 \
Training Set R-squared: 0.9713132015763644 \
Test Set MAD: 172.67858666666672\
Test Set MSE: 47159.16804325623\
Test Set RMSE: 217.16161733431676\
Test Set R-squared: 0.9094522829075513\
Population MAD: 152.85791896\
Population MSE: 38045.911329152754\
Population RMSE: 195.05361142299506\
Population R-squared: 0.9269704185623958

We can see that the population metrics is almost the same as the test metrics.
